{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "import keras \n",
    "from keras.preprocessing.sequence import pad_sequences #2.2.4\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn \n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_chinese = BertTokenizer.from_pretrained(\"bert-base-chinese\", do_lower_case=False)\n",
    "tag_values = ['O',\n",
    " 'B_person_name',\n",
    " 'M_person_name',\n",
    " 'E_person_name',\n",
    " 'PAD']\n",
    "tag2idx = { 'O': 0,\n",
    "           'B_person_name': 1,\n",
    "           'M_person_name': 2,\n",
    "           'E_person_name': 3,\n",
    "           'PAD': 4}\n",
    "# Model1 \n",
    "PATH = 'step1_1output_bertmode_step1_ner.pth'\n",
    "model = torch.load(PATH)\n",
    "model.eval()\n",
    "# Model class must be defined somewhere\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "PATH = 'step3_output_bert_senti.pth'#'bertmode_step2_senti_model1_目前成效最好.pth'\n",
    "\n",
    "model2 = torch.load(PATH)\n",
    "model2.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def split_content(x):\n",
    "    \n",
    "    if len(x)<=500:\n",
    "        return [x]\n",
    "    elif (len(x)>=500) and(len(x)<1000):\n",
    "        return [ x[:500+3],x[500-6:] ]\n",
    "    \n",
    "    elif (len(x)>=1000) and (len(x)<1500):\n",
    "        return [ x[:500+3],x[500-3:1000+3] ,x[1000-3:]]\n",
    "    \n",
    "    elif (len(x)>=1500) and (len(x)<2000):\n",
    "        return [ x[:500+3],x[500-3:1000+3] ,x[1000-3:1500+3], x[1500-3:]]\n",
    "    \n",
    "    else:\n",
    "        return [x[:500+3],x[500-3:1000+3] ,x[1000-3:1500+3], x[1500-3:2000-3] , x[2000-3:2000-3+500] ]\n",
    "\n",
    "def find_all_indexes(input_str, search_str):\n",
    "    l1 = []\n",
    "    length = len(input_str)\n",
    "    index = 0\n",
    "    while index < length:\n",
    "        i = input_str.find(search_str, index)\n",
    "        if i == -1:\n",
    "            return l1\n",
    "        l1.append(i)\n",
    "        index = i + 1\n",
    "    return l1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(txt):\n",
    "    \"\"\" Predict your model result\n",
    "    @param article (str): a news article\n",
    "    @returns prediction (list): a list of name\n",
    "    \"\"\"\n",
    "\n",
    "    ####### PUT YOUR MODEL INFERENCING CODE HERE #######\n",
    "    test_sentence = split_content(txt)\n",
    "    row_name = []\n",
    "    # model1 predict 產生文章中的 name list\n",
    "    for sentence in test_sentence:\n",
    "        # bert預測\n",
    "        tokenized_sentence = tokenizer_chinese.encode(sentence)\n",
    "        input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "        tokens = tokenizer_chinese.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "        new_tokens, new_labels = [], []\n",
    "        for token, label_idx in zip(tokens, label_indices[0]):\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                new_labels.append(tag_values[label_idx])#ex:['O','O','O','O',...]\n",
    "                new_tokens.append(token)# ex:['[CLS]', '益', '公', '司', '債', '或', '新',...]\n",
    "        texto=''\n",
    "        for i in range(len(new_labels)):\n",
    "            if new_labels[i] !='O':\n",
    "                texto+= new_tokens[i]\n",
    "            else:\n",
    "                texto+='O' #'OOO張堯勇OOOOOOOOOOOOO'\n",
    "        for i in texto.split('O'):\n",
    "            if len(i)>1:#['張堯勇', '張堯勇']，單一個字或者空白的會被削去\n",
    "                 row_name.append(i)\n",
    "    uniq_name = list(set(row_name)) #['鄭心芸', '巴菲特', '詹姆斯·西蒙斯', '堯勇', '索羅斯', '張堯勇']\n",
    "    \n",
    "    \n",
    "    article_truncate = [] # 依據名字擷取的文章\n",
    "    article_tags = [] # 該人是否是黑名單\n",
    "    article_tags_name = [] # 該人是誰\n",
    "    article_len = len(txt)#文章長度\n",
    "    article_content = txt#文章內文\n",
    "    all_name_list = uniq_name# 找出共有幾個人\n",
    "    for j in all_name_list: # 第i篇文章的第j個人\n",
    "        name_index = find_all_indexes(article_content, j )#出現在文章的index\n",
    "        for k in name_index:\n",
    "            article_tags_name.append(j)\n",
    "            txt2 = txt[  np.clip(k-min(100 , int(article_len*0.2))\n",
    "                                             , 0 ,999999\n",
    "                                                  )  : \n",
    "                                  np.clip(k-min(50 , int(article_len*0.2))\n",
    "                                             , 0 , 99999\n",
    "                                         )+250\n",
    "                                        ].replace( j , '李家賢') \n",
    "            article_truncate.append(txt2)\n",
    "    df = pd.DataFrame(data={'article_truncate':article_truncate,\n",
    "                  'article_tags_name':article_tags_name})\n",
    "    # model2 predict\n",
    "    prediction_list = []\n",
    "    for i in range(len(df)):\n",
    "        review_text = df.iloc[i].article_truncate\n",
    "        encoded_review = tokenizer_chinese.encode_plus(\n",
    "        review_text,\n",
    "        max_length=100,#MAX_LEN,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt')\n",
    "        input_ids = encoded_review['input_ids'].to(device)\n",
    "        attention_mask = encoded_review['attention_mask'].to(device)\n",
    "        output = model2(input_ids, attention_mask)\n",
    "        _, prediction = torch.max(output, dim=1)\n",
    "        prediction_list.append(prediction.tolist()[0])\n",
    "        #print(prediction.tolist()[0])\n",
    "    df['prediction'] = prediction_list   \n",
    "    if len(df)==0:\n",
    "        prediction = []\n",
    "    else:\n",
    "\n",
    "        df_group = df.groupby('article_tags_name').mean().apply(lambda x:np.where(x>=0.35,1,0)) # 0.35 是 threshold能調整\n",
    "        prediction = df_group[df_group.prediction==1].index.tolist()\n",
    "    ####################################################\n",
    "    #prediction = _check_datatype_to_list(prediction)\n",
    "    return list(set(prediction)) ,uniq_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''台北地檢署偵辦國安特勤私菸案，歷經1個月多馬不停蹄、抽絲剝繭調查，今偵查終結，將於10點30分開記者會說明。因涉案人數眾多，定調以「嚴辦首謀」、「從犯從輕」為原則，這波偵結對象是以「在押、首謀」為主，擬以貪汙治罪條例第4條第1項第4款「以公用運輸工具裝運違禁物品或漏稅物罪」、第6條第1項第4款「圖利罪」起訴國安局及華航人員。專案小組認定國安局少校吳宗憲、張恒嘉與華航空品處前副總邱彰信、協理于堯、代理組長黃川禎等人涉犯貪汙治罪條例，另買菸大戶上士駕駛劉尊彰、民間人士李宗原等人涉違反稅捐稽徵法，依法起訴。其餘單純團購人員則另簽分偵辦。7月22日，總統蔡英文結束12天「自由民主永續之旅」返台，調查局以行政檢查方式，攔查國安局官員隨團走私9800條菸，金額高達645萬元，創桃園機場夾帶走私菸最大數量紀錄。全案爆發後，北檢立刻組成專案小組。檢方先親赴吳、張位於總統官邸附近侍衛室的宿舍、辦公室，調取電磁紀錄、筆記本、存摺等。鞏固物證後，再大規模約談訂購名單上的買家，查明購菸目的屬自用、代購、轉售。檢調清查華航是誰下令開放免稅菸品無限訂購，幫助特勤人員逃漏稅，8月1日約談華航前資深副總羅雅美、空品處前副總邱彰信等人。複訊後，檢方命邱以70萬元交保、羅雅美則訊後請回、華航空品處協理于堯坦承聽從邱指令辦事，50萬元交保。為釐清華航「機邊交貨」的緣起和運作模式，專案小組向上溯源，傳喚空品處代理組長黃川禎、空品處前後任經理沈珉等華航人員，另傳喚透過總統府侍衛室及國安局訂私菸的民眾共10餘人，訊後全數請回。最後階段，檢調陸續清查訂購數百條的「大戶」，約談華航年輕空服員白梓佑、總統專機御廚田佳宜、華航空品處資深員工陳穎彥、徐世立等人，訊後依違反稅捐稽徵法，各以5萬元至20萬元交保。台北地檢署。圖／本報資料照片facebook'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, uniq_name = predict(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['徐世立',\n",
       " '沈珉',\n",
       " '黃川禎',\n",
       " '雅美',\n",
       " '邱彰信',\n",
       " '劉尊彰',\n",
       " '白梓佑',\n",
       " '李宗原',\n",
       " '于堯',\n",
       " '陳穎彥',\n",
       " '張恒嘉',\n",
       " '蔡英文',\n",
       " '羅雅美',\n",
       " '吳宗憲',\n",
       " '田佳宜']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     雅美\n",
       "7    羅雅美\n",
       "dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = pd.Series(prediction)\n",
    "s1[s1.str.contains('雅美', regex=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
